{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils = {\n",
    "    \"resenet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n",
    "}\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3*3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Resnet-B\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检测分支"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把这个函数理解为类型转换函数，将一个不可训练的数据类型Tensor转换成可以训练的数据类型parameter,并将这个parameter绑定到这个module里面；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleExp(nn.Module):\n",
    "    \"\"\"\n",
    "    定义一个指数缩放模块\n",
    "    \"\"\"\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(ScaleExp, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.tensor([init_value], dtype=torch.float32))  # nn.parameter 将一个不可训练的tensor转换乘可以训练的类型parameter,并将这个parameter绑定到这个module里面； \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.exp(x * self.scale)  # 乘一个可以训练的缩放因子 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsCntRegHead(nn.Module):\n",
    "    \"\"\"检测分支\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, class_num, GN=True, cnt_on_reg=True, prior=0.01):\n",
    "        super(ClsCntRegHead, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.class_num = class_num\n",
    "        self.cnt_on_reg = cnt_on_reg\n",
    "        \n",
    "        # 1 ========================从fpn到head的侧连===========================\n",
    "        cls_branch = []\n",
    "        reg_branch = []\n",
    "        for i in range(4):\n",
    "            # cls_branch: conv--gn--relu  这里的卷积都不改变图像尺寸\n",
    "            cls_branch.append(nn.Conv2d(in_channel, in_channel, kernel_size=3, padding=1, bias = True))\n",
    "            if GN:\n",
    "                reg_branch.append(nn.GroupNorm(32, in_channel))\n",
    "            cls_branch.append(nn.ReLU(True))\n",
    "            \n",
    "            # reg_branch: conv--gn--relu  这里的卷积都不改变图像尺寸\n",
    "            reg_branch.append(nn.Conv2d(in_channel, in_channel, kernel_size=3, padding=1, bias = True))\n",
    "            if GN:\n",
    "                reg_branch.append(nn.GroupNorm(32, in_channel))\n",
    "            reg_branch.append(nn.ReLU(True))\n",
    "        \n",
    "        # 1.1 分类网络分支\n",
    "        self.cls_conv = nn.Sequential(*cls_branch)  # 将cls_branch中的参数，一个个拆解成独立的参数；列表或者元组之前加一个*，字典前加两个**\n",
    "        # 1.2 回归网络分支\n",
    "        self.reg_conv = nn.Sequential(*reg_branch)\n",
    "        print(' Bypasses of the detection head;')\n",
    "        print(self.cls_conv, '\\n')\n",
    "        print(self.reg_conv)\n",
    "        # =======================================================================\n",
    "        \n",
    "        # 2 ========================网络输出=====================================\n",
    "        # 2.1 网络分类路径输出\n",
    "        self.cls_logits = nn.Conv2d(in_channel, class_num, kernel_size=3, padding=1)\n",
    "        # 2.2 网络回归路径输出\n",
    "        self.reg_pred = nn.Conv2d(in_channel, 4, kernel_size=3, padding=1)\n",
    "        # 2.3 目标中心输出\n",
    "        self.cnt_logits = nn.Conv2d(in_channel, 1, kernel_size=3, padding=1)\n",
    "        # =======================================================================\n",
    "        \n",
    "        # 3. 网络参数初始化\n",
    "        self.apply(self.init_conv_RandomNormal)\n",
    "        nn.init.constant_(self.cls_logits.bias, - math.log((1-prior)/prior))\n",
    "        \n",
    "        # 4. 实例化五个缩放层\n",
    "        self.scale_exp = nn.ModuleList([ScaleExp(1.0) for _ in range(5)])        \n",
    "        \n",
    "    def init_conv_RandomNormal(self, module, std=0.01):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            nn.init.normal_(module.weight, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            inputs ([[p3~p7]): _description_\n",
    "        \"\"\"\n",
    "        cls_logits = []\n",
    "        cnt_logits = []\n",
    "        reg_preds = []\n",
    "        for index, P in enumerate(inputs):\n",
    "            # 所有的P层都经过cls_conv(),所以这里，模块cls_conv()的参数是共享的。\n",
    "            cls_conv_out = self.cls_conv(P)\n",
    "            # 所有的p层都经过reg_conv(),所以这里，模块reg_conv()的参数是共享的。\n",
    "            reg_conv_out = self.reg_conv(P)\n",
    "            \n",
    "            cls_logits.append(self.cls_logits(cls_conv_out))\n",
    "            \n",
    "            if not self.cnt_on_reg:  # 中心回归放在哪一个分支上，是cls_conv_out, 还是reg_conv_out分支\n",
    "                cnt_logits.append(self.cnt_logits(cls_conv_out))\n",
    "            else:\n",
    "                cnt_logits.append(self.cnt_logits(reg_conv_out))  # 中心回归默认放在reg_conv_out分支\n",
    "            \n",
    "            reg_preds.append(self.scale_exp[index](self.reg_pred(reg_conv_out)))\n",
    "        return cls_logits, cnt_logits, reg_preds  # 每一个返回的list都有5个分量，对应P3~P7的卷积输出；\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bypasses of the detection head;\n",
      "Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      ") \n",
      "\n",
      "Sequential(\n",
      "  (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (14): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (15): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "head = ClsCntRegHead(256, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('IMX500')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6898330a772008c0efb32b6e51c3fb8f855e3228c7dbee5aae70bc6dd7211e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
